2017-09-20 16:53:03,405 - TensorMol - INFO - Searching for Installed Optional Packages...
2017-09-20 16:53:03,592 - TensorMol - DEBUG - Pyscf has been found
2017-09-20 16:53:03,593 - TensorMol - DEBUG - MolEmb has been found, Orthogonalizing Radial Basis.
2017-09-20 16:53:03,686 - TensorMol - DEBUG - Tensorflow version 1.3.0 has been found
2017-09-20 16:53:03,686 - TensorMol - DEBUG - Found 32 CPUs to thread over... 
2017-09-20 16:53:03,686 - TensorMol - DEBUG - TensorMol ready...
2017-09-20 16:53:03,687 - TensorMol - DEBUG - TMPARAMS----------
2017-09-20 16:53:03,687 - TensorMol - DEBUG - InNormRoutine:None
NebNumBeads:10
MDdt:0.2
SH_MAXNR:12
EEOrder:2
RotAvOutputs:1
EESwitchFunc:CosLR
dig_ngrid:20
AN1_r_Rc:4.6
GradWeight:0.01
AN1_num_a_Rs:8
BlurRadius:0.05
tf_prec:tf.float32
MDFieldT0:3.0
AN1_r_Rs:[ 0.       0.14375  0.2875   0.43125  0.575    0.71875  0.8625   1.00625
  1.15     1.29375  1.4375   1.58125  1.725    1.86875  2.0125   2.15625
  2.3      2.44375  2.5875   2.73125  2.875    3.01875  3.1625   3.30625
  3.45     3.59375  3.7375   3.88125  4.025    4.16875  4.3125   4.45625]
momentum:0.9
learning_rate_dipole:0.0001
CellWidth:15.0
AN1_a_Rc:3.1
SH_NRAD:12
TransformSet:1
OptMomentumDecay:0.8
MDFieldTau:1.2
AN1_a_Rs:[ 0.      0.3875  0.775   1.1625  1.55    1.9375  2.325   2.7125]
learning_rate_energy:1e-05
RBFS:[[  0.24666382   0.37026093]
 [  0.42773663   0.47058503]
 [  0.5780647    0.47249905]
 [  0.63062578   0.60452219]
 [  1.30332807   1.2604625 ]
 [  2.2          2.4       ]
 [  4.4          2.4       ]
 [  6.6          2.4       ]
 [  8.8          2.4       ]
 [ 11.           2.4       ]
 [ 13.2          2.4       ]
 [ 15.4          2.4       ]]
AddEcc:True
RotateSet:0
MDIrForceMin:False
MDAnnealT0:20.0
EEVdw:True
MxMemPerElement:16000
dens_dir:./densities/
OptMaxBFGS:7
NModePts:10
Profiling:False
DSFAlpha:0.15
MxTimePerElement:36000
OptPrintLvl:1
DiisSize:20
Qchem_RIMP2_Block:$rem
   jobtype   sp
   method   rimp2
   MAX_SCF_CYCLES  200
   basis   cc-pvtz
   aux_basis rimp2-cc-pvtz
   symmetry   false
   INCFOCK 0
   thresh 12
   SCF_CONVERGENCE 12
$end

EECutoffOff:15.0
OptMaxStep:0.1
SRBF:[[  1.81906966e+01  -3.69177402e+01   2.68791669e+01  -4.04922928e+00
    5.46997770e-01  -7.15507988e-01   6.72174790e-01  -4.28899004e-01
    2.26745041e-01  -1.02137384e-01   3.66922198e-02  -8.15713944e-03]
 [ -3.69177402e+01   8.81214505e+01  -6.69233362e+01   9.18208065e+00
    3.79230428e-01  -1.01108684e+00   1.14262432e+00  -9.11839845e-01
    6.25605530e-01  -3.71652718e-01   1.74543716e-01  -4.93467194e-02]
 [  2.68791669e+01  -6.69233362e+01   6.02278630e+01  -1.43485162e+01
    8.07241994e-01  -8.33774442e-02  -3.83878968e-01   6.24680223e-01
   -5.84174516e-01   4.02934898e-01  -2.03026930e-01   5.92257835e-02]
 [ -4.04922928e+00   9.18208065e+00  -1.43485162e+01   9.62893922e+00
   -2.16580709e+00   1.00563808e+00  -3.68068038e-01  -1.30640248e-01
    2.70339307e-01  -2.18377632e-01   1.15095618e-01  -3.39114586e-02]
 [  5.46997770e-01   3.79230428e-01   8.07241994e-01  -2.16580709e+00
    4.40677735e+00  -4.97370683e+00   4.25901530e+00  -2.61658326e+00
    1.43385927e+00  -7.12639238e-01   2.92861594e-01  -7.52345766e-02]
 [ -7.15507988e-01  -1.01108684e+00  -8.33774442e-02   1.00563808e+00
   -4.97370683e+00   8.78405565e+00  -8.68061760e+00   6.00786857e+00
   -3.57750590e+00   1.87066552e+00  -7.91152233e-01   2.06460901e-01]
 [  6.72174790e-01   1.14262432e+00  -3.83878968e-01  -3.68068038e-01
    4.25901530e+00  -8.68061760e+00   1.06391480e+01  -8.29923039e+00
    5.29507051e+00  -2.87738767e+00   1.24192967e+00  -3.27471020e-01]
 [ -4.28899004e-01  -9.11839845e-01   6.24680223e-01  -1.30640248e-01
   -2.61658326e+00   6.00786857e+00  -8.29923039e+00   8.42764364e+00
   -6.17580884e+00   3.59893179e+00  -1.60962320e+00   4.32156925e-01]
 [  2.26745041e-01   6.25605530e-01  -5.84174516e-01   2.70339307e-01
    1.43385927e+00  -3.57750590e+00   5.29507051e+00  -6.17580884e+00
    6.23863859e+00  -4.25785159e+00   2.05511617e+00  -5.73079190e-01]
 [ -1.02137384e-01  -3.71652718e-01   4.02934898e-01  -2.18377632e-01
   -7.12639238e-01   1.87066552e+00  -2.87738767e+00   3.59893179e+00
   -4.25785159e+00   4.33749127e+00  -2.53352836e+00   7.76237474e-01]
 [  3.66922198e-02   1.74543716e-01  -2.03026930e-01   1.15095618e-01
    2.92861594e-01  -7.91152233e-01   1.24192967e+00  -1.60962320e+00
    2.05511617e+00  -2.53352836e+00   2.54585703e+00  -1.00659371e+00]
 [ -8.15713944e-03  -4.93467194e-02   5.92257835e-02  -3.39114586e-02
   -7.52345766e-02   2.06460901e-01  -3.27471020e-01   4.32156925e-01
   -5.73079190e-01   7.76237474e-01  -1.00659371e+00   9.79330932e-01]]
batch_size:1000
NebMaxBFGS:12
EEdr:1.0
AN1_a_As:[ 0.          0.78539816  1.57079633  2.35619449  3.14159265  3.92699082
  4.71238898  5.49778714]
MetaMaxBumps:2500
EECutoff:5.0
GSSearchAlpha:0.001
Poly_Width:4.6
MDAnnealTF:300.0
MBE_ORDER:3
NeuronType:relu
CheckLevel:1
OptThresh:0.0001
SH_LMAX:4
RemoveInvariant:True
SH_ORTH:1
NDistorts:5
OctahedralAveraging:0
MDThermostat:None
dig_SamplingType:Smooth
Classify:False
MAX_ATOMIC_NUMBER:10
NebK:0.01
results_dir:./results/
AN1_num_a_As:8
RandomizeData:True
MDUpdateCharges:True
MDV0:Random
DipoleScalar:1.0
sets_dir:./datasets/
OptMomentum:0.0
EECutoffOn:4.4
OptMaxCycles:20
MDAnnealKickBack:1.0
GauSHSm12:[[  1.81906966e+01  -2.68706936e-11  -1.61148908e-11 ...,   1.02790032e-14
   -2.04310789e-15  -1.58840522e-14]
 [  2.67014507e-11   1.81906966e+01   9.68707337e-10 ...,   5.34267129e-13
   -1.49855117e-13  -2.50732269e-13]
 [  1.58067317e-11  -1.24323751e-09   1.81906966e+01 ...,   6.66144223e-13
   -4.46375142e-13  -3.72495871e-13]
 ..., 
 [ -2.37736762e-15  -3.26571886e-13  -2.66242323e-13 ...,   9.79330932e-01
    8.42028137e-15  -1.57422539e-14]
 [  4.80559370e-16  -1.65016232e-12  -3.87420131e-14 ...,  -7.79614810e-15
    9.79330932e-01   6.55151122e-15]
 [  4.04176391e-15  -6.99563996e-13   1.04615969e-12 ...,  -4.07442795e-15
   -2.28856027e-14   9.79330932e-01]]
Embedded_Charge_Order:2
ANES:[ 0.96763427  1.          1.          1.          1.          2.14952757
  1.95145955  2.01797792]
hidden3:512
Erf_Width:0.2
max_steps:1001
AN1_eta:4.0
hidden2:512
hidden1:512
MDMaxStep:20000
MDLogTrajectory:True
GIT_REVISION:180c92d

max_checkpoints:1
ChopTo:None
GradScalar:1.0
MetaBowlK:0.0
EEOn:True
AN1_zeta:8.0
AN1_num_r_Rs:32
MDFieldVec:[ 1.  0.  0.]
TestRatio:0.2
OptStepSize:0.1
HiddenLayers:[200, 200, 200]
OutNormRoutine:None
MDFieldFreq:0.833333333333
learning_rate:0.001
log_dir:./logs/
GoK:0.05
test_freq:10
MDTemp:300.0
MDAnnealSteps:1000
MDFieldAmp:0.0

2017-09-20 16:53:03,690 - TensorMol - DEBUG - TMPARAMS~~~~~~~~~~
2017-09-20 16:53:03,757 - TensorMol - INFO - Loaded, 2500 molecules 65000 Atoms total [1 6 7] Types 
2017-09-20 16:53:03,777 - TensorMol - INFO - TensorMolData.type: mol
2017-09-20 16:53:03,777 - TensorMol - INFO - TensorMolData.dig.name: ANI1_Sym_Direct
2017-09-20 16:53:03,778 - TensorMol - INFO - NMols in TensorMolData.set: 2500
2017-09-20 16:53:03,799 - TensorMol - DEBUG - Assigning Activation... relu
2017-09-20 16:53:03,799 - TensorMol - INFO - self.learning_rate: 1e-05
2017-09-20 16:53:03,799 - TensorMol - INFO - self.batch_size: 200
2017-09-20 16:53:03,799 - TensorMol - INFO - self.max_steps: 401
2017-09-20 16:53:03,799 - TensorMol - DEBUG - Assigning Activation... relu
2017-09-20 16:53:03,808 - TensorMol - DEBUG - LastTrainMol in TensorMolData: 2000
2017-09-20 16:53:03,808 - TensorMol - DEBUG - NTestMols in TensorMolData: 500
2017-09-20 16:53:03,808 - TensorMol - INFO - -------------------- 
2017-09-20 16:53:03,808 - TensorMol - INFO - Transformer Information 
2017-09-20 16:53:03,808 - TensorMol - INFO - self.innorm: None
2017-09-20 16:53:03,808 - TensorMol - INFO - self.outnorm: None
2017-09-20 16:53:03,808 - TensorMol - INFO - -------------------- 
2017-09-20 16:53:03,808 - TensorMol - INFO - MolInstance.inshape None MolInstance.outshape None
2017-09-20 16:53:03,809 - TensorMol - DEBUG - Raised Instance: Mol_nicotine_aimd_rand_train_ANI1_Sym_Direct_RawBP_Grad
2017-09-20 16:53:03,809 - TensorMol - INFO - running the TFMolInstance.train()
2017-09-20 16:53:58,001 - TensorMol - INFO - step:       0  duration: 51.10613  train loss: 8.8779228403  energy_loss: 8.8207993883  grad_loss: 0.0571234520
2017-09-20 16:54:47,689 - TensorMol - INFO - step:       1  duration: 49.68605  train loss: 8.2739942957  energy_loss: 8.2157316192  grad_loss: 0.0582626765
2017-09-20 16:55:38,165 - TensorMol - INFO - step:       2  duration: 50.47389  train loss: 7.6989362899  energy_loss: 7.6388842054  grad_loss: 0.0600520845
2017-09-20 16:56:28,006 - TensorMol - INFO - step:       3  duration: 49.83922  train loss: 7.1437539297  energy_loss: 7.0811743159  grad_loss: 0.0625796138
2017-09-20 16:57:17,091 - TensorMol - INFO - step:       4  duration: 49.08373  train loss: 6.6008626255  energy_loss: 6.5356509753  grad_loss: 0.0652116503
2017-09-20 16:58:06,263 - TensorMol - INFO - step:       5  duration: 49.17017  train loss: 6.0641943978  energy_loss: 5.9958863521  grad_loss: 0.0683080457
2017-09-20 16:58:56,033 - TensorMol - INFO - step:       6  duration: 49.76871  train loss: 5.5322723963  energy_loss: 5.4609766438  grad_loss: 0.0712957525
2017-09-20 16:59:46,101 - TensorMol - INFO - step:       7  duration: 50.06587  train loss: 5.0031074293  energy_loss: 4.9281812551  grad_loss: 0.0749261742
2017-09-20 17:00:35,544 - TensorMol - INFO - step:       8  duration: 49.44151  train loss: 4.4817120378  energy_loss: 4.4029636162  grad_loss: 0.0787484216
2017-09-20 17:01:25,011 - TensorMol - INFO - step:       9  duration: 49.46495  train loss: 3.9653309697  energy_loss: 3.8820668061  grad_loss: 0.0832641636
2017-09-20 17:02:14,455 - TensorMol - INFO - step:      10  duration: 49.44276  train loss: 3.4601345640  energy_loss: 3.3713704885  grad_loss: 0.0887640755
2017-09-20 17:02:24,348 - TensorMol - INFO - step:      10  duration: 9.89087  train loss: 3.1780474411  energy_loss: 3.0860244734  grad_loss: 0.0920229677
2017-09-20 17:03:13,976 - TensorMol - INFO - step:      11  duration: 49.26690  train loss: 2.9699033324  energy_loss: 2.8750624098  grad_loss: 0.0948409226
2017-09-20 17:04:03,376 - TensorMol - INFO - step:      12  duration: 49.39763  train loss: 2.4984679750  energy_loss: 2.3965898003  grad_loss: 0.1018781746
2017-09-20 17:04:53,628 - TensorMol - INFO - step:      13  duration: 50.25137  train loss: 2.0588652975  energy_loss: 1.9492333157  grad_loss: 0.1096319819
2017-09-20 17:05:42,792 - TensorMol - INFO - step:      14  duration: 49.16201  train loss: 1.6542084819  energy_loss: 1.5356246586  grad_loss: 0.1185838233
2017-09-20 17:06:31,874 - TensorMol - INFO - step:      15  duration: 49.08181  train loss: 1.2951557515  energy_loss: 1.1672337126  grad_loss: 0.1279220390
2017-09-20 17:07:21,114 - TensorMol - INFO - step:      16  duration: 49.23825  train loss: 0.9865351276  energy_loss: 0.8476276068  grad_loss: 0.1389075208
2017-09-20 17:08:10,422 - TensorMol - INFO - step:      17  duration: 49.30563  train loss: 0.7367603610  energy_loss: 0.5863635040  grad_loss: 0.1503968571
2017-09-20 17:09:00,351 - TensorMol - INFO - step:      18  duration: 49.92793  train loss: 0.5430504968  energy_loss: 0.3808219779  grad_loss: 0.1622285188
2017-09-20 17:09:49,698 - TensorMol - INFO - step:      19  duration: 49.34564  train loss: 0.4042168000  energy_loss: 0.2314671348  grad_loss: 0.1727496652
2017-09-20 17:10:39,180 - TensorMol - INFO - step:      20  duration: 49.48053  train loss: 0.3125328204  energy_loss: 0.1310634308  grad_loss: 0.1814693896
2017-09-20 17:10:48,922 - TensorMol - INFO - step:      20  duration: 9.74022  train loss: 0.2745310768  energy_loss: 0.0890386734  grad_loss: 0.1854924034
2017-09-20 17:11:38,635 - TensorMol - INFO - step:      21  duration: 49.46544  train loss: 0.2575720454  energy_loss: 0.0689607178  grad_loss: 0.1886113276
2017-09-20 17:12:28,142 - TensorMol - INFO - step:      22  duration: 49.50520  train loss: 0.2281877082  energy_loss: 0.0352601593  grad_loss: 0.1929275488
2017-09-20 17:13:17,385 - TensorMol - INFO - step:      23  duration: 49.24171  train loss: 0.2136144213  energy_loss: 0.0180548607  grad_loss: 0.1955595607
2017-09-20 17:14:06,772 - TensorMol - INFO - step:      24  duration: 49.38583  train loss: 0.2058786472  energy_loss: 0.0104904734  grad_loss: 0.1953881738
2017-09-20 17:14:56,463 - TensorMol - INFO - step:      25  duration: 49.68874  train loss: 0.2014509012  energy_loss: 0.0072602431  grad_loss: 0.1941906582
2017-09-20 17:15:45,609 - TensorMol - INFO - step:      26  duration: 49.14496  train loss: 0.1970606746  energy_loss: 0.0060978638  grad_loss: 0.1909628108
2017-09-20 17:16:35,347 - TensorMol - INFO - step:      27  duration: 49.73678  train loss: 0.1934037790  energy_loss: 0.0056288050  grad_loss: 0.1877749740
2017-09-20 17:17:24,453 - TensorMol - INFO - step:      28  duration: 49.10421  train loss: 0.1898274629  energy_loss: 0.0054635998  grad_loss: 0.1843638631
2017-09-20 17:18:13,878 - TensorMol - INFO - step:      29  duration: 49.42427  train loss: 0.1859961631  energy_loss: 0.0054516933  grad_loss: 0.1805444697
2017-09-20 17:19:02,862 - TensorMol - INFO - step:      30  duration: 48.98275  train loss: 0.1825676638  energy_loss: 0.0053522294  grad_loss: 0.1772154344
2017-09-20 17:19:12,783 - TensorMol - INFO - step:      30  duration: 9.92017  train loss: 0.1800851400  energy_loss: 0.0050527952  grad_loss: 0.1750323448
2017-09-20 17:20:02,815 - TensorMol - INFO - step:      31  duration: 49.66256  train loss: 0.1788674305  energy_loss: 0.0054015432  grad_loss: 0.1734658872
2017-09-20 17:20:52,024 - TensorMol - INFO - step:      32  duration: 49.20749  train loss: 0.1757113045  energy_loss: 0.0053015946  grad_loss: 0.1704097100
2017-09-20 17:21:41,158 - TensorMol - INFO - step:      33  duration: 49.13238  train loss: 0.1721068555  energy_loss: 0.0052360300  grad_loss: 0.1668708255
2017-09-20 17:22:31,040 - TensorMol - INFO - step:      34  duration: 49.88029  train loss: 0.1691128786  energy_loss: 0.0051237591  grad_loss: 0.1639891196
2017-09-20 17:23:20,727 - TensorMol - INFO - step:      35  duration: 49.68684  train loss: 0.1654115005  energy_loss: 0.0050812871  grad_loss: 0.1603302134
2017-09-20 17:24:09,933 - TensorMol - INFO - step:      36  duration: 49.20458  train loss: 0.1622400592  energy_loss: 0.0049974206  grad_loss: 0.1572426385
2017-09-20 17:24:59,319 - TensorMol - INFO - step:      37  duration: 49.38435  train loss: 0.1591134507  energy_loss: 0.0048971089  grad_loss: 0.1542163418
2017-09-20 17:25:48,567 - TensorMol - INFO - step:      38  duration: 49.24706  train loss: 0.1557824690  energy_loss: 0.0048455270  grad_loss: 0.1509369420
2017-09-20 17:26:37,897 - TensorMol - INFO - step:      39  duration: 49.32835  train loss: 0.1528071742  energy_loss: 0.0047110874  grad_loss: 0.1480960868
2017-09-20 17:27:27,328 - TensorMol - INFO - step:      40  duration: 49.42983  train loss: 0.1496026793  energy_loss: 0.0047144034  grad_loss: 0.1448882759
2017-09-20 17:27:37,171 - TensorMol - INFO - step:      40  duration: 9.84090  train loss: 0.1477346633  energy_loss: 0.0043869416  grad_loss: 0.1433477217
2017-09-20 17:28:26,808 - TensorMol - INFO - step:      41  duration: 49.34659  train loss: 0.1468113272  energy_loss: 0.0046184619  grad_loss: 0.1421928653
2017-09-20 17:29:16,085 - TensorMol - INFO - step:      42  duration: 49.27487  train loss: 0.1436096765  energy_loss: 0.0045552562  grad_loss: 0.1390544203
2017-09-20 17:30:05,347 - TensorMol - INFO - step:      43  duration: 49.26014  train loss: 0.1409565567  energy_loss: 0.0044699570  grad_loss: 0.1364865997
2017-09-20 17:30:54,777 - TensorMol - INFO - step:      44  duration: 49.42892  train loss: 0.1376061818  energy_loss: 0.0044267466  grad_loss: 0.1331794352
2017-09-20 17:31:44,134 - TensorMol - INFO - step:      45  duration: 49.35496  train loss: 0.1347982703  energy_loss: 0.0043580765  grad_loss: 0.1304401938
2017-09-20 17:32:32,993 - TensorMol - INFO - step:      46  duration: 48.85885  train loss: 0.1320053851  energy_loss: 0.0042657591  grad_loss: 0.1277396260
2017-09-20 17:33:21,980 - TensorMol - INFO - step:      47  duration: 48.98517  train loss: 0.1290707780  energy_loss: 0.0042178054  grad_loss: 0.1248529726
2017-09-20 17:34:11,250 - TensorMol - INFO - step:      48  duration: 49.26930  train loss: 0.1264684431  energy_loss: 0.0040933725  grad_loss: 0.1223750705
2017-09-20 17:35:00,551 - TensorMol - INFO - step:      49  duration: 49.29973  train loss: 0.1236920194  energy_loss: 0.0040750773  grad_loss: 0.1196169421
2017-09-20 17:35:49,851 - TensorMol - INFO - step:      50  duration: 49.29797  train loss: 0.1213297854  energy_loss: 0.0039870958  grad_loss: 0.1173426896
2017-09-20 17:35:59,650 - TensorMol - INFO - step:      50  duration: 9.79791  train loss: 0.1195772871  energy_loss: 0.0037558926  grad_loss: 0.1158213945
2017-09-20 17:36:49,682 - TensorMol - INFO - step:      51  duration: 49.65429  train loss: 0.1186030699  energy_loss: 0.0039234132  grad_loss: 0.1146796567
2017-09-20 17:37:39,233 - TensorMol - INFO - step:      52  duration: 49.54872  train loss: 0.1164233409  energy_loss: 0.0038591811  grad_loss: 0.1125641598
2017-09-20 17:38:29,540 - TensorMol - INFO - step:      53  duration: 50.30512  train loss: 0.1136494740  energy_loss: 0.0038133092  grad_loss: 0.1098361649
2017-09-20 17:39:18,933 - TensorMol - INFO - step:      54  duration: 49.39198  train loss: 0.1114373317  energy_loss: 0.0037618102  grad_loss: 0.1076755214
2017-09-20 17:40:08,091 - TensorMol - INFO - step:      55  duration: 49.15651  train loss: 0.1091611003  energy_loss: 0.0036835657  grad_loss: 0.1054775346
2017-09-20 17:40:57,614 - TensorMol - INFO - step:      56  duration: 49.52199  train loss: 0.1067216588  energy_loss: 0.0036486862  grad_loss: 0.1030729726
2017-09-20 17:41:47,350 - TensorMol - INFO - step:      57  duration: 49.73469  train loss: 0.1046015176  energy_loss: 0.0035499166  grad_loss: 0.1010516010
2017-09-20 17:42:37,379 - TensorMol - INFO - step:      58  duration: 50.02763  train loss: 0.1023821222  energy_loss: 0.0035317173  grad_loss: 0.0988504048
2017-09-20 17:43:26,673 - TensorMol - INFO - step:      59  duration: 49.29203  train loss: 0.1005307023  energy_loss: 0.0034683790  grad_loss: 0.0970623232
2017-09-20 17:44:16,238 - TensorMol - INFO - step:      60  duration: 49.56359  train loss: 0.0983759961  energy_loss: 0.0034182816  grad_loss: 0.0949577146
2017-09-20 17:44:26,005 - TensorMol - INFO - step:      60  duration: 9.76558  train loss: 0.0971376717  energy_loss: 0.0032568674  grad_loss: 0.0938808043
2017-09-20 17:45:15,334 - TensorMol - INFO - step:      61  duration: 49.05706  train loss: 0.0967271538  energy_loss: 0.0033767998  grad_loss: 0.0933503540
2017-09-20 17:46:04,572 - TensorMol - INFO - step:      62  duration: 49.23675  train loss: 0.0945100782  energy_loss: 0.0033337794  grad_loss: 0.0911762988
2017-09-20 17:46:54,061 - TensorMol - INFO - step:      63  duration: 49.48772  train loss: 0.0928035773  energy_loss: 0.0032980514  grad_loss: 0.0895055260
2017-09-20 17:47:43,437 - TensorMol - INFO - step:      64  duration: 49.37478  train loss: 0.0910394134  energy_loss: 0.0032292268  grad_loss: 0.0878101866
2017-09-20 17:48:33,412 - TensorMol - INFO - step:      65  duration: 49.97370  train loss: 0.0891928439  energy_loss: 0.0032093437  grad_loss: 0.0859835002
2017-09-20 17:49:22,845 - TensorMol - INFO - step:      66  duration: 49.43190  train loss: 0.0875718023  energy_loss: 0.0031371196  grad_loss: 0.0844346828
2017-09-20 17:50:11,633 - TensorMol - INFO - step:      67  duration: 48.78662  train loss: 0.0858358784  energy_loss: 0.0031238897  grad_loss: 0.0827119887
2017-09-20 17:51:01,052 - TensorMol - INFO - step:      68  duration: 49.41782  train loss: 0.0843554352  energy_loss: 0.0030782647  grad_loss: 0.0812771705
2017-09-20 17:51:50,909 - TensorMol - INFO - step:      69  duration: 49.85483  train loss: 0.0825731859  energy_loss: 0.0030360312  grad_loss: 0.0795371547
2017-09-20 17:52:39,897 - TensorMol - INFO - step:      70  duration: 48.98636  train loss: 0.0811539509  energy_loss: 0.0030062603  grad_loss: 0.0781476906
2017-09-20 17:52:49,762 - TensorMol - INFO - step:      70  duration: 9.86424  train loss: 0.0799379531  energy_loss: 0.0028795285  grad_loss: 0.0770584246
2017-09-20 17:53:39,136 - TensorMol - INFO - step:      71  duration: 49.08166  train loss: 0.0793300929  energy_loss: 0.0029627358  grad_loss: 0.0763673572
2017-09-20 17:54:28,680 - TensorMol - INFO - step:      72  duration: 49.54219  train loss: 0.0780056007  energy_loss: 0.0029340822  grad_loss: 0.0750715185
2017-09-20 17:55:18,231 - TensorMol - INFO - step:      73  duration: 49.55053  train loss: 0.0766309740  energy_loss: 0.0028694589  grad_loss: 0.0737615151
2017-09-20 17:56:07,648 - TensorMol - INFO - step:      74  duration: 49.41514  train loss: 0.0751795744  energy_loss: 0.0028555211  grad_loss: 0.0723240533
2017-09-20 17:56:57,218 - TensorMol - INFO - step:      75  duration: 49.56918  train loss: 0.0739501825  energy_loss: 0.0027984621  grad_loss: 0.0711517204
2017-09-20 17:57:46,738 - TensorMol - INFO - step:      76  duration: 49.51905  train loss: 0.0726219340  energy_loss: 0.0027863277  grad_loss: 0.0698356063
2017-09-20 17:58:35,963 - TensorMol - INFO - step:      77  duration: 49.22344  train loss: 0.0715350574  energy_loss: 0.0027518683  grad_loss: 0.0687831891
2017-09-20 17:59:25,360 - TensorMol - INFO - step:      78  duration: 49.39572  train loss: 0.0702476856  energy_loss: 0.0027160793  grad_loss: 0.0675316063
2017-09-20 18:00:14,712 - TensorMol - INFO - step:      79  duration: 49.35008  train loss: 0.0692932769  energy_loss: 0.0026943958  grad_loss: 0.0665988811
2017-09-20 18:01:03,710 - TensorMol - INFO - step:      80  duration: 48.99708  train loss: 0.0679040064  energy_loss: 0.0026529400  grad_loss: 0.0652510664
2017-09-20 18:01:13,663 - TensorMol - INFO - step:      80  duration: 9.95222  train loss: 0.0672698323  energy_loss: 0.0025666883  grad_loss: 0.0647031439
2017-09-20 18:02:03,536 - TensorMol - INFO - step:      81  duration: 49.41510  train loss: 0.0669672459  energy_loss: 0.0026317449  grad_loss: 0.0643355011
2017-09-20 18:02:52,455 - TensorMol - INFO - step:      82  duration: 48.91757  train loss: 0.0658999423  energy_loss: 0.0025716103  grad_loss: 0.0633283320
2017-09-20 18:03:41,840 - TensorMol - INFO - step:      83  duration: 49.38418  train loss: 0.0647745914  energy_loss: 0.0025595889  grad_loss: 0.0622150025
2017-09-20 18:04:31,636 - TensorMol - INFO - step:      84  duration: 49.79493  train loss: 0.0638428735  energy_loss: 0.0025113867  grad_loss: 0.0613314868
2017-09-20 18:05:21,237 - TensorMol - INFO - step:      85  duration: 49.59929  train loss: 0.0628187157  energy_loss: 0.0024991878  grad_loss: 0.0603195279
2017-09-20 18:06:10,493 - TensorMol - INFO - step:      86  duration: 49.25466  train loss: 0.0619916001  energy_loss: 0.0024707354  grad_loss: 0.0595208647
2017-09-20 18:06:59,972 - TensorMol - INFO - step:      87  duration: 49.47793  train loss: 0.0610004329  energy_loss: 0.0024382936  grad_loss: 0.0585621394
2017-09-20 18:07:49,391 - TensorMol - INFO - step:      88  duration: 49.41779  train loss: 0.0602637428  energy_loss: 0.0024176076  grad_loss: 0.0578461353
2017-09-20 18:08:38,622 - TensorMol - INFO - step:      89  duration: 49.22889  train loss: 0.0591675550  energy_loss: 0.0023760738  grad_loss: 0.0567914813
2017-09-20 18:09:27,522 - TensorMol - INFO - step:      90  duration: 48.89937  train loss: 0.0585208901  energy_loss: 0.0023585432  grad_loss: 0.0561623469
2017-09-20 18:09:37,347 - TensorMol - INFO - step:      90  duration: 9.82283  train loss: 0.0579609548  energy_loss: 0.0022877514  grad_loss: 0.0556732034
2017-09-20 18:10:27,568 - TensorMol - INFO - step:      91  duration: 49.96213  train loss: 0.0577012706  energy_loss: 0.0023036233  grad_loss: 0.0553976473
2017-09-20 18:11:17,092 - TensorMol - INFO - step:      92  duration: 49.52215  train loss: 0.0568358797  energy_loss: 0.0022949693  grad_loss: 0.0545409104
2017-09-20 18:12:06,437 - TensorMol - INFO - step:      93  duration: 49.34360  train loss: 0.0561660539  energy_loss: 0.0022554097  grad_loss: 0.0539106442
2017-09-20 18:12:56,013 - TensorMol - INFO - step:      94  duration: 49.57518  train loss: 0.0554198385  energy_loss: 0.0022450071  grad_loss: 0.0531748314
2017-09-20 18:13:45,206 - TensorMol - INFO - step:      95  duration: 49.19062  train loss: 0.0548375724  energy_loss: 0.0022204613  grad_loss: 0.0526171110
2017-09-20 18:14:34,832 - TensorMol - INFO - step:      96  duration: 49.62497  train loss: 0.0540940153  energy_loss: 0.0021947617  grad_loss: 0.0518992536
2017-09-20 18:15:24,575 - TensorMol - INFO - step:      97  duration: 49.74262  train loss: 0.0535774853  energy_loss: 0.0021764923  grad_loss: 0.0514009930
2017-09-20 18:16:13,993 - TensorMol - INFO - step:      98  duration: 49.41607  train loss: 0.0527385035  energy_loss: 0.0021413396  grad_loss: 0.0505971639
2017-09-20 18:17:03,870 - TensorMol - INFO - step:      99  duration: 49.87547  train loss: 0.0523098817  energy_loss: 0.0021302272  grad_loss: 0.0501796545
2017-09-20 18:17:53,009 - TensorMol - INFO - step:     100  duration: 49.13812  train loss: 0.0517139599  energy_loss: 0.0020822636  grad_loss: 0.0496316963
2017-09-20 18:18:02,952 - TensorMol - INFO - step:     100  duration: 9.94203  train loss: 0.0513123772  energy_loss: 0.0020493485  grad_loss: 0.0492630287
2017-09-20 18:18:52,507 - TensorMol - INFO - step:     101  duration: 49.19785  train loss: 0.0510476659  energy_loss: 0.0020774592  grad_loss: 0.0489702067
2017-09-20 18:19:41,221 - TensorMol - INFO - step:     102  duration: 48.71251  train loss: 0.0505266260  energy_loss: 0.0020460555  grad_loss: 0.0484805705
2017-09-20 18:20:30,545 - TensorMol - INFO - step:     103  duration: 49.32250  train loss: 0.0499576639  energy_loss: 0.0020417853  grad_loss: 0.0479158787
2017-09-20 18:21:20,098 - TensorMol - INFO - step:     104  duration: 49.55171  train loss: 0.0495144640  energy_loss: 0.0020241946  grad_loss: 0.0474902694
2017-09-20 18:22:09,334 - TensorMol - INFO - step:     105  duration: 49.23467  train loss: 0.0489337736  energy_loss: 0.0020066443  grad_loss: 0.0469271293
2017-09-20 18:22:58,298 - TensorMol - INFO - step:     106  duration: 48.96253  train loss: 0.0484972183  energy_loss: 0.0019898922  grad_loss: 0.0465073261
2017-09-20 18:23:47,963 - TensorMol - INFO - step:     107  duration: 49.66308  train loss: 0.0477931888  energy_loss: 0.0019613455  grad_loss: 0.0458318433
2017-09-20 18:24:37,086 - TensorMol - INFO - step:     108  duration: 49.12224  train loss: 0.0474555580  energy_loss: 0.0019541657  grad_loss: 0.0455013923
2017-09-20 18:25:26,333 - TensorMol - INFO - step:     109  duration: 49.24478  train loss: 0.0469550105  energy_loss: 0.0019112681  grad_loss: 0.0450437424
2017-09-20 18:26:15,663 - TensorMol - INFO - step:     110  duration: 49.32889  train loss: 0.0463838958  energy_loss: 0.0019102062  grad_loss: 0.0444736896
2017-09-20 18:26:25,380 - TensorMol - INFO - step:     110  duration: 9.71557  train loss: 0.0462078152  energy_loss: 0.0018669288  grad_loss: 0.0443408863
2017-09-20 18:27:15,827 - TensorMol - INFO - step:     111  duration: 50.21301  train loss: 0.0459755812  energy_loss: 0.0018855166  grad_loss: 0.0440900647
2017-09-20 18:28:06,268 - TensorMol - INFO - step:     112  duration: 50.44012  train loss: 0.0455376887  energy_loss: 0.0018850292  grad_loss: 0.0436526595
2017-09-20 18:28:55,681 - TensorMol - INFO - step:     113  duration: 49.41075  train loss: 0.0451897661  energy_loss: 0.0018707235  grad_loss: 0.0433190426
2017-09-20 18:29:45,179 - TensorMol - INFO - step:     114  duration: 49.49677  train loss: 0.0447400661  energy_loss: 0.0018585559  grad_loss: 0.0428815101
2017-09-20 18:30:34,790 - TensorMol - INFO - step:     115  duration: 49.60957  train loss: 0.0444040300  energy_loss: 0.0018399521  grad_loss: 0.0425640778
2017-09-20 18:31:24,656 - TensorMol - INFO - step:     116  duration: 49.86464  train loss: 0.0438439138  energy_loss: 0.0018155460  grad_loss: 0.0420283678
2017-09-20 18:32:13,990 - TensorMol - INFO - step:     117  duration: 49.33257  train loss: 0.0436002175  energy_loss: 0.0018087389  grad_loss: 0.0417914786
2017-09-20 18:33:04,292 - TensorMol - INFO - step:     118  duration: 50.30005  train loss: 0.0432001842  energy_loss: 0.0017683532  grad_loss: 0.0414318310
2017-09-20 18:33:55,267 - TensorMol - INFO - step:     119  duration: 50.97341  train loss: 0.0427395170  energy_loss: 0.0017671948  grad_loss: 0.0409723223
2017-09-20 18:34:45,063 - TensorMol - INFO - step:     120  duration: 49.79455  train loss: 0.0424019321  energy_loss: 0.0017457824  grad_loss: 0.0406561497
2017-09-20 18:34:54,905 - TensorMol - INFO - step:     120  duration: 9.84097  train loss: 0.0421434326  energy_loss: 0.0017061588  grad_loss: 0.0404372738
2017-09-20 18:35:44,622 - TensorMol - INFO - step:     121  duration: 49.40159  train loss: 0.0420363674  energy_loss: 0.0017475550  grad_loss: 0.0402888124
2017-09-20 18:36:33,986 - TensorMol - INFO - step:     122  duration: 49.36172  train loss: 0.0417228420  energy_loss: 0.0017339402  grad_loss: 0.0399889019
2017-09-20 18:37:23,585 - TensorMol - INFO - step:     123  duration: 49.59787  train loss: 0.0413553404  energy_loss: 0.0017253357  grad_loss: 0.0396300047
2017-09-20 18:38:13,501 - TensorMol - INFO - step:     124  duration: 49.91442  train loss: 0.0410672481  energy_loss: 0.0017064225  grad_loss: 0.0393608256
2017-09-20 18:39:03,298 - TensorMol - INFO - step:     125  duration: 49.79512  train loss: 0.0405947662  energy_loss: 0.0016885695  grad_loss: 0.0389061968
2017-09-20 18:39:52,637 - TensorMol - INFO - step:     126  duration: 49.33778  train loss: 0.0403983218  energy_loss: 0.0016851155  grad_loss: 0.0387132063
2017-09-20 18:40:42,027 - TensorMol - INFO - step:     127  duration: 49.38934  train loss: 0.0400644065  energy_loss: 0.0016510727  grad_loss: 0.0384133338
2017-09-20 18:41:31,818 - TensorMol - INFO - step:     128  duration: 49.78964  train loss: 0.0396709780  energy_loss: 0.0016526720  grad_loss: 0.0380183060
2017-09-20 18:42:23,334 - TensorMol - INFO - step:     129  duration: 51.51475  train loss: 0.0393666420  energy_loss: 0.0016363969  grad_loss: 0.0377302451
2017-09-20 18:43:13,753 - TensorMol - INFO - step:     130  duration: 50.41676  train loss: 0.0390550636  energy_loss: 0.0016417372  grad_loss: 0.0374133264
2017-09-20 18:43:23,471 - TensorMol - INFO - step:     130  duration: 9.71669  train loss: 0.0387184392  energy_loss: 0.0015847604  grad_loss: 0.0371336788
2017-09-20 18:44:13,381 - TensorMol - INFO - step:     131  duration: 49.54425  train loss: 0.0387739793  energy_loss: 0.0016298874  grad_loss: 0.0371440919
2017-09-20 18:45:03,272 - TensorMol - INFO - step:     132  duration: 49.88944  train loss: 0.0384535621  energy_loss: 0.0016239415  grad_loss: 0.0368296207
2017-09-20 18:45:53,981 - TensorMol - INFO - step:     133  duration: 50.70742  train loss: 0.0381784683  energy_loss: 0.0016050469  grad_loss: 0.0365734213
2017-09-20 18:46:45,420 - TensorMol - INFO - step:     134  duration: 51.43745  train loss: 0.0377706056  energy_loss: 0.0015878428  grad_loss: 0.0361827628
2017-09-20 18:47:35,144 - TensorMol - INFO - step:     135  duration: 49.72283  train loss: 0.0376448047  energy_loss: 0.0015825551  grad_loss: 0.0360622496
2017-09-20 18:48:24,816 - TensorMol - INFO - step:     136  duration: 49.67104  train loss: 0.0373898505  energy_loss: 0.0015501938  grad_loss: 0.0358396567
2017-09-20 18:49:15,166 - TensorMol - INFO - step:     137  duration: 50.34840  train loss: 0.0370331876  energy_loss: 0.0015517164  grad_loss: 0.0354814712
2017-09-20 18:50:05,432 - TensorMol - INFO - step:     138  duration: 50.26475  train loss: 0.0367139612  energy_loss: 0.0015380111  grad_loss: 0.0351759501
2017-09-20 18:50:55,104 - TensorMol - INFO - step:     139  duration: 49.67162  train loss: 0.0364066576  energy_loss: 0.0015447598  grad_loss: 0.0348618978
2017-09-20 18:51:44,860 - TensorMol - INFO - step:     140  duration: 49.75434  train loss: 0.0361380839  energy_loss: 0.0015330992  grad_loss: 0.0346049846
2017-09-20 18:51:54,955 - TensorMol - INFO - step:     140  duration: 10.09327  train loss: 0.0357825627  energy_loss: 0.0014763735  grad_loss: 0.0343061891
2017-09-20 18:52:46,392 - TensorMol - INFO - step:     141  duration: 51.10699  train loss: 0.0358361007  energy_loss: 0.0015295655  grad_loss: 0.0343065352
2017-09-20 18:53:36,566 - TensorMol - INFO - step:     142  duration: 50.17123  train loss: 0.0355845104  energy_loss: 0.0015110684  grad_loss: 0.0340734420
2017-09-20 18:54:26,300 - TensorMol - INFO - step:     143  duration: 49.73189  train loss: 0.0351792985  energy_loss: 0.0014991701  grad_loss: 0.0336801284
2017-09-20 18:55:15,962 - TensorMol - INFO - step:     144  duration: 49.66046  train loss: 0.0350041509  energy_loss: 0.0014975820  grad_loss: 0.0335065689
2017-09-20 18:56:05,420 - TensorMol - INFO - step:     145  duration: 49.45698  train loss: 0.0347051026  energy_loss: 0.0014693529  grad_loss: 0.0332357497
2017-09-20 18:56:55,086 - TensorMol - INFO - step:     146  duration: 49.66506  train loss: 0.0343517601  energy_loss: 0.0014705156  grad_loss: 0.0328812445
2017-09-20 18:57:48,156 - TensorMol - INFO - step:     147  duration: 53.06854  train loss: 0.0340615663  energy_loss: 0.0014578003  grad_loss: 0.0326037660
2017-09-20 18:58:39,625 - TensorMol - INFO - step:     148  duration: 51.46778  train loss: 0.0337727481  energy_loss: 0.0014661652  grad_loss: 0.0323065828
2017-09-20 18:59:31,557 - TensorMol - INFO - step:     149  duration: 51.93055  train loss: 0.0334957547  energy_loss: 0.0014527939  grad_loss: 0.0320429608
2017-09-20 19:00:22,852 - TensorMol - INFO - step:     150  duration: 51.29378  train loss: 0.0331986638  energy_loss: 0.0014460755  grad_loss: 0.0317525883
2017-09-20 19:00:33,093 - TensorMol - INFO - step:     150  duration: 10.23953  train loss: 0.0329073045  energy_loss: 0.0013773722  grad_loss: 0.0315299324
2017-09-20 19:01:23,277 - TensorMol - INFO - step:     151  duration: 49.80023  train loss: 0.0330110729  energy_loss: 0.0014226502  grad_loss: 0.0315884227
2017-09-20 19:02:13,145 - TensorMol - INFO - step:     152  duration: 49.86604  train loss: 0.0326670846  energy_loss: 0.0014100138  grad_loss: 0.0312570708
2017-09-20 19:03:02,980 - TensorMol - INFO - step:     153  duration: 49.83372  train loss: 0.0324963274  energy_loss: 0.0014070078  grad_loss: 0.0310893196
2017-09-20 19:03:53,975 - TensorMol - INFO - step:     154  duration: 50.99377  train loss: 0.0321989167  energy_loss: 0.0013795869  grad_loss: 0.0308193297
2017-09-20 19:04:44,535 - TensorMol - INFO - step:     155  duration: 50.55913  train loss: 0.0318473678  energy_loss: 0.0013791128  grad_loss: 0.0304682551
2017-09-20 19:05:34,671 - TensorMol - INFO - step:     156  duration: 50.13425  train loss: 0.0315658424  energy_loss: 0.0013668899  grad_loss: 0.0301989525
2017-09-20 19:06:24,679 - TensorMol - INFO - step:     157  duration: 50.00692  train loss: 0.0312993266  energy_loss: 0.0013756276  grad_loss: 0.0299236990
2017-09-20 19:07:14,626 - TensorMol - INFO - step:     158  duration: 49.94676  train loss: 0.0310848620  energy_loss: 0.0013622104  grad_loss: 0.0297226515
2017-09-20 19:08:06,937 - TensorMol - INFO - step:     159  duration: 52.30967  train loss: 0.0308279498  energy_loss: 0.0013581373  grad_loss: 0.0294698125
2017-09-20 19:08:58,732 - TensorMol - INFO - step:     160  duration: 51.79342  train loss: 0.0306036613  energy_loss: 0.0013370257  grad_loss: 0.0292666355
2017-09-20 19:09:08,590 - TensorMol - INFO - step:     160  duration: 9.85711  train loss: 0.0302214546  energy_loss: 0.0012814665  grad_loss: 0.0289399881
2017-09-20 19:09:58,977 - TensorMol - INFO - step:     161  duration: 50.06847  train loss: 0.0302766936  energy_loss: 0.0013266722  grad_loss: 0.0289500214
2017-09-20 19:10:52,472 - TensorMol - INFO - step:     162  duration: 53.49356  train loss: 0.0301329189  energy_loss: 0.0013229389  grad_loss: 0.0288099800
2017-09-20 19:11:43,625 - TensorMol - INFO - step:     163  duration: 51.15135  train loss: 0.0298854744  energy_loss: 0.0012966784  grad_loss: 0.0285887960
2017-09-20 19:12:34,369 - TensorMol - INFO - step:     164  duration: 50.74263  train loss: 0.0295914043  energy_loss: 0.0012947441  grad_loss: 0.0282966603
2017-09-20 19:13:25,291 - TensorMol - INFO - step:     165  duration: 50.92086  train loss: 0.0293282969  energy_loss: 0.0012824104  grad_loss: 0.0280458865
2017-09-20 19:14:15,338 - TensorMol - INFO - step:     166  duration: 50.04584  train loss: 0.0290942220  energy_loss: 0.0012904219  grad_loss: 0.0278038001
2017-09-20 19:15:06,506 - TensorMol - INFO - step:     167  duration: 51.16644  train loss: 0.0288862189  energy_loss: 0.0012750912  grad_loss: 0.0276111276
2017-09-20 19:15:55,986 - TensorMol - INFO - step:     168  duration: 49.47856  train loss: 0.0286616170  energy_loss: 0.0012708078  grad_loss: 0.0273908092
2017-09-20 19:16:45,441 - TensorMol - INFO - step:     169  duration: 49.45383  train loss: 0.0284606509  energy_loss: 0.0012486852  grad_loss: 0.0272119657
2017-09-20 19:17:34,872 - TensorMol - INFO - step:     170  duration: 49.42882  train loss: 0.0281631621  energy_loss: 0.0012393392  grad_loss: 0.0269238229
2017-09-20 19:17:44,737 - TensorMol - INFO - step:     170  duration: 9.86448  train loss: 0.0278982396  energy_loss: 0.0011839340  grad_loss: 0.0267143056
2017-09-20 19:18:36,249 - TensorMol - INFO - step:     171  duration: 51.17325  train loss: 0.0280223654  energy_loss: 0.0012346876  grad_loss: 0.0267876778
2017-09-20 19:19:25,763 - TensorMol - INFO - step:     172  duration: 49.51203  train loss: 0.0278059178  energy_loss: 0.0012096645  grad_loss: 0.0265962532
2017-09-20 19:20:15,830 - TensorMol - INFO - step:     173  duration: 50.06631  train loss: 0.0275283450  energy_loss: 0.0012065807  grad_loss: 0.0263217643
2017-09-20 19:21:06,598 - TensorMol - INFO - step:     174  duration: 50.76616  train loss: 0.0272779736  energy_loss: 0.0011950246  grad_loss: 0.0260829490
2017-09-20 19:21:56,723 - TensorMol - INFO - step:     175  duration: 50.12376  train loss: 0.0270443998  energy_loss: 0.0012029276  grad_loss: 0.0258414723
2017-09-20 19:22:46,306 - TensorMol - INFO - step:     176  duration: 49.58201  train loss: 0.0268409491  energy_loss: 0.0011865189  grad_loss: 0.0256544302
2017-09-20 19:23:36,169 - TensorMol - INFO - step:     177  duration: 49.86177  train loss: 0.0266135712  energy_loss: 0.0011822303  grad_loss: 0.0254313409
2017-09-20 19:24:25,925 - TensorMol - INFO - step:     178  duration: 49.75390  train loss: 0.0263987515  energy_loss: 0.0011600269  grad_loss: 0.0252387246
2017-09-20 19:25:16,363 - TensorMol - INFO - step:     179  duration: 50.43704  train loss: 0.0260939945  energy_loss: 0.0011518816  grad_loss: 0.0249421129
2017-09-20 19:26:06,009 - TensorMol - INFO - step:     180  duration: 49.64455  train loss: 0.0259309188  energy_loss: 0.0011466674  grad_loss: 0.0247842514
2017-09-20 19:26:15,950 - TensorMol - INFO - step:     180  duration: 9.93888  train loss: 0.0256432567  energy_loss: 0.0010886365  grad_loss: 0.0245546201
2017-09-20 19:27:05,988 - TensorMol - INFO - step:     181  duration: 49.73269  train loss: 0.0256769050  energy_loss: 0.0011231298  grad_loss: 0.0245537752
2017-09-20 19:27:55,550 - TensorMol - INFO - step:     182  duration: 49.55955  train loss: 0.0253962147  energy_loss: 0.0011187304  grad_loss: 0.0242774842
2017-09-20 19:28:46,559 - TensorMol - INFO - step:     183  duration: 51.00724  train loss: 0.0251752651  energy_loss: 0.0011075368  grad_loss: 0.0240677283
2017-09-20 19:29:36,689 - TensorMol - INFO - step:     184  duration: 50.12855  train loss: 0.0249233352  energy_loss: 0.0011149753  grad_loss: 0.0238083599
2017-09-20 19:30:28,479 - TensorMol - INFO - step:     185  duration: 51.78927  train loss: 0.0247384964  energy_loss: 0.0010983106  grad_loss: 0.0236401858
2017-09-20 19:31:18,934 - TensorMol - INFO - step:     186  duration: 50.45281  train loss: 0.0245304298  energy_loss: 0.0010935143  grad_loss: 0.0234369154
2017-09-20 19:32:08,362 - TensorMol - INFO - step:     187  duration: 49.42634  train loss: 0.0243343689  energy_loss: 0.0010707023  grad_loss: 0.0232636666
2017-09-20 19:32:58,643 - TensorMol - INFO - step:     188  duration: 50.27918  train loss: 0.0240445942  energy_loss: 0.0010630235  grad_loss: 0.0229815707
2017-09-20 19:33:47,982 - TensorMol - INFO - step:     189  duration: 49.33696  train loss: 0.0238909073  energy_loss: 0.0010557834  grad_loss: 0.0228351239
2017-09-20 19:34:38,013 - TensorMol - INFO - step:     190  duration: 50.02920  train loss: 0.0236677768  energy_loss: 0.0010326514  grad_loss: 0.0226351254
2017-09-20 19:34:48,178 - TensorMol - INFO - step:     190  duration: 10.16375  train loss: 0.0234555241  energy_loss: 0.0009849097  grad_loss: 0.0224706144
2017-09-20 19:35:38,225 - TensorMol - INFO - step:     191  duration: 49.69635  train loss: 0.0234804616  energy_loss: 0.0010242634  grad_loss: 0.0224561982
2017-09-20 19:36:28,880 - TensorMol - INFO - step:     192  duration: 50.65325  train loss: 0.0233302672  energy_loss: 0.0010128175  grad_loss: 0.0223174496
2017-09-20 19:37:18,203 - TensorMol - INFO - step:     193  duration: 49.32141  train loss: 0.0231407808  energy_loss: 0.0010192743  grad_loss: 0.0221215065
2017-09-20 19:38:08,976 - TensorMol - INFO - step:     194  duration: 50.77191  train loss: 0.0229840649  energy_loss: 0.0010018246  grad_loss: 0.0219822403
2017-09-20 19:39:00,735 - TensorMol - INFO - step:     195  duration: 51.75775  train loss: 0.0227759234  energy_loss: 0.0009972015  grad_loss: 0.0217787220
2017-09-20 19:39:51,651 - TensorMol - INFO - step:     196  duration: 50.91371  train loss: 0.0225997805  energy_loss: 0.0009740237  grad_loss: 0.0216257568
2017-09-20 19:40:41,087 - TensorMol - INFO - step:     197  duration: 49.43563  train loss: 0.0223517564  energy_loss: 0.0009674549  grad_loss: 0.0213843015
2017-09-20 19:41:31,143 - TensorMol - INFO - step:     198  duration: 50.05393  train loss: 0.0222427431  energy_loss: 0.0009595684  grad_loss: 0.0212831747
2017-09-20 19:42:20,647 - TensorMol - INFO - step:     199  duration: 49.50290  train loss: 0.0220627119  energy_loss: 0.0009398690  grad_loss: 0.0211228429
2017-09-20 19:43:10,889 - TensorMol - INFO - step:     200  duration: 50.24088  train loss: 0.0218540944  energy_loss: 0.0009333244  grad_loss: 0.0209207700
2017-09-20 19:43:20,811 - TensorMol - INFO - step:     200  duration: 9.92066  train loss: 0.0217184854  energy_loss: 0.0008895831  grad_loss: 0.0208289023
2017-09-20 19:44:10,739 - TensorMol - INFO - step:     201  duration: 49.60298  train loss: 0.0216591973  energy_loss: 0.0009232885  grad_loss: 0.0207359088
2017-09-20 19:45:02,668 - TensorMol - INFO - step:     202  duration: 51.92828  train loss: 0.0214692362  energy_loss: 0.0009290954  grad_loss: 0.0205401408
2017-09-20 19:45:53,386 - TensorMol - INFO - step:     203  duration: 50.71590  train loss: 0.0213578494  energy_loss: 0.0009114688  grad_loss: 0.0204463805
2017-09-20 19:46:43,157 - TensorMol - INFO - step:     204  duration: 49.77010  train loss: 0.0211733047  energy_loss: 0.0009075541  grad_loss: 0.0202657506
2017-09-20 19:47:33,999 - TensorMol - INFO - step:     205  duration: 50.84072  train loss: 0.0210016294  energy_loss: 0.0008852786  grad_loss: 0.0201163508
2017-09-20 19:48:23,708 - TensorMol - INFO - step:     206  duration: 49.70790  train loss: 0.0208025831  energy_loss: 0.0008802514  grad_loss: 0.0199223317
2017-09-20 19:49:13,054 - TensorMol - INFO - step:     207  duration: 49.34418  train loss: 0.0206945548  energy_loss: 0.0008723936  grad_loss: 0.0198221613
2017-09-20 19:50:02,417 - TensorMol - INFO - step:     208  duration: 49.36219  train loss: 0.0205263185  energy_loss: 0.0008549405  grad_loss: 0.0196713781
2017-09-20 19:50:53,609 - TensorMol - INFO - step:     209  duration: 51.19023  train loss: 0.0203672978  energy_loss: 0.0008478148  grad_loss: 0.0195194830
2017-09-20 19:51:43,877 - TensorMol - INFO - step:     210  duration: 50.26741  train loss: 0.0202127544  energy_loss: 0.0008381430  grad_loss: 0.0193746114
2017-09-20 19:51:55,561 - TensorMol - INFO - step:     210  duration: 11.68189  train loss: 0.0201208985  energy_loss: 0.0007984017  grad_loss: 0.0193224969
2017-09-20 19:52:45,358 - TensorMol - INFO - step:     211  duration: 49.48753  train loss: 0.0200564958  energy_loss: 0.0008437438  grad_loss: 0.0192127520
2017-09-20 19:53:35,044 - TensorMol - INFO - step:     212  duration: 49.68387  train loss: 0.0199258978  energy_loss: 0.0008264443  grad_loss: 0.0190994535
2017-09-20 19:54:24,520 - TensorMol - INFO - step:     213  duration: 49.47506  train loss: 0.0197389173  energy_loss: 0.0008234063  grad_loss: 0.0189155110
2017-09-20 19:55:16,487 - TensorMol - INFO - step:     214  duration: 51.96523  train loss: 0.0195923762  energy_loss: 0.0008010706  grad_loss: 0.0187913056
2017-09-20 19:56:06,503 - TensorMol - INFO - step:     215  duration: 50.01446  train loss: 0.0193793620  energy_loss: 0.0007971943  grad_loss: 0.0185821677
2017-09-20 19:56:56,476 - TensorMol - INFO - step:     216  duration: 49.97140  train loss: 0.0192572757  energy_loss: 0.0007888871  grad_loss: 0.0184683887
2017-09-20 19:57:46,569 - TensorMol - INFO - step:     217  duration: 50.09221  train loss: 0.0191076451  energy_loss: 0.0007733894  grad_loss: 0.0183342556
2017-09-20 19:58:36,915 - TensorMol - INFO - step:     218  duration: 50.34389  train loss: 0.0189260399  energy_loss: 0.0007662667  grad_loss: 0.0181597732
2017-09-20 19:59:27,864 - TensorMol - INFO - step:     219  duration: 50.94853  train loss: 0.0187195898  energy_loss: 0.0007570909  grad_loss: 0.0179624989
2017-09-20 20:00:17,542 - TensorMol - INFO - step:     220  duration: 49.67627  train loss: 0.0185605971  energy_loss: 0.0007604736  grad_loss: 0.0178001235
2017-09-20 20:00:27,360 - TensorMol - INFO - step:     220  duration: 9.81635  train loss: 0.0184736905  energy_loss: 0.0007099310  grad_loss: 0.0177637595
2017-09-20 20:01:19,390 - TensorMol - INFO - step:     221  duration: 51.66363  train loss: 0.0183964644  energy_loss: 0.0007413609  grad_loss: 0.0176551035
2017-09-20 20:02:09,940 - TensorMol - INFO - step:     222  duration: 50.54859  train loss: 0.0181543163  energy_loss: 0.0007380149  grad_loss: 0.0174163015
2017-09-20 20:02:59,590 - TensorMol - INFO - step:     223  duration: 49.64912  train loss: 0.0179702737  energy_loss: 0.0007137295  grad_loss: 0.0172565442
2017-09-20 20:03:49,392 - TensorMol - INFO - step:     224  duration: 49.80111  train loss: 0.0177585097  energy_loss: 0.0007077693  grad_loss: 0.0170507403
2017-09-20 20:04:39,158 - TensorMol - INFO - step:     225  duration: 49.76420  train loss: 0.0176385255  energy_loss: 0.0006971271  grad_loss: 0.0169413984
2017-09-20 20:05:28,757 - TensorMol - INFO - step:     226  duration: 49.59809  train loss: 0.0174923491  energy_loss: 0.0006820438  grad_loss: 0.0168103054
2017-09-20 20:06:19,062 - TensorMol - INFO - step:     227  duration: 50.30383  train loss: 0.0173046987  energy_loss: 0.0006732937  grad_loss: 0.0166314050
2017-09-20 20:07:08,256 - TensorMol - INFO - step:     228  duration: 49.19262  train loss: 0.0171183655  energy_loss: 0.0006647261  grad_loss: 0.0164536394
2017-09-20 20:07:58,494 - TensorMol - INFO - step:     229  duration: 50.23688  train loss: 0.0169962968  energy_loss: 0.0006678371  grad_loss: 0.0163284598
2017-09-20 20:08:48,195 - TensorMol - INFO - step:     230  duration: 49.69966  train loss: 0.0168753651  energy_loss: 0.0006501300  grad_loss: 0.0162252350
2017-09-20 20:08:58,120 - TensorMol - INFO - step:     230  duration: 9.92456  train loss: 0.0168078344  energy_loss: 0.0006146822  grad_loss: 0.0161931522
2017-09-20 20:09:48,129 - TensorMol - INFO - step:     231  duration: 49.71362  train loss: 0.0166951043  energy_loss: 0.0006489954  grad_loss: 0.0160461089
2017-09-20 20:10:39,172 - TensorMol - INFO - step:     232  duration: 51.04157  train loss: 0.0165761819  energy_loss: 0.0006289781  grad_loss: 0.0159472039
2017-09-20 20:11:30,186 - TensorMol - INFO - step:     233  duration: 51.01175  train loss: 0.0164143909  energy_loss: 0.0006270651  grad_loss: 0.0157873258
2017-09-20 20:12:19,422 - TensorMol - INFO - step:     234  duration: 49.23496  train loss: 0.0162677508  energy_loss: 0.0006183591  grad_loss: 0.0156493917
2017-09-20 20:13:09,832 - TensorMol - INFO - step:     235  duration: 50.40864  train loss: 0.0161571189  energy_loss: 0.0006060373  grad_loss: 0.0155510816
2017-09-20 20:13:59,743 - TensorMol - INFO - step:     236  duration: 49.91103  train loss: 0.0160116554  energy_loss: 0.0005973013  grad_loss: 0.0154143541
2017-09-20 20:14:49,562 - TensorMol - INFO - step:     237  duration: 49.81782  train loss: 0.0158282663  energy_loss: 0.0005896988  grad_loss: 0.0152385675
2017-09-20 20:15:39,601 - TensorMol - INFO - step:     238  duration: 50.03759  train loss: 0.0157047056  energy_loss: 0.0005930615  grad_loss: 0.0151116441
2017-09-20 20:16:29,790 - TensorMol - INFO - step:     239  duration: 50.18730  train loss: 0.0155751163  energy_loss: 0.0005761145  grad_loss: 0.0149990018
2017-09-20 20:17:19,961 - TensorMol - INFO - step:     240  duration: 50.16985  train loss: 0.0153676533  energy_loss: 0.0005757622  grad_loss: 0.0147918911
2017-09-20 20:17:30,163 - TensorMol - INFO - step:     240  duration: 10.20004  train loss: 0.0153293371  energy_loss: 0.0005368578  grad_loss: 0.0147924793
2017-09-20 20:18:19,849 - TensorMol - INFO - step:     241  duration: 49.34973  train loss: 0.0152415908  energy_loss: 0.0005571804  grad_loss: 0.0146844104
2017-09-20 20:19:10,154 - TensorMol - INFO - step:     242  duration: 50.30372  train loss: 0.0151004566  energy_loss: 0.0005569838  grad_loss: 0.0145434728
2017-09-20 20:19:59,912 - TensorMol - INFO - step:     243  duration: 49.75602  train loss: 0.0149731264  energy_loss: 0.0005496678  grad_loss: 0.0144234586
2017-09-20 20:20:49,632 - TensorMol - INFO - step:     244  duration: 49.71886  train loss: 0.0148748007  energy_loss: 0.0005406091  grad_loss: 0.0143341916
2017-09-20 20:21:39,451 - TensorMol - INFO - step:     245  duration: 49.81769  train loss: 0.0147443329  energy_loss: 0.0005331444  grad_loss: 0.0142111884
2017-09-20 20:22:29,652 - TensorMol - INFO - step:     246  duration: 50.20036  train loss: 0.0145866363  energy_loss: 0.0005274831  grad_loss: 0.0140591532
2017-09-20 20:23:19,109 - TensorMol - INFO - step:     247  duration: 49.45516  train loss: 0.0144746708  energy_loss: 0.0005320363  grad_loss: 0.0139426345
2017-09-20 20:24:08,440 - TensorMol - INFO - step:     248  duration: 49.32948  train loss: 0.0143788112  energy_loss: 0.0005167254  grad_loss: 0.0138620858
2017-09-20 20:24:58,129 - TensorMol - INFO - step:     249  duration: 49.68790  train loss: 0.0142217397  energy_loss: 0.0005177759  grad_loss: 0.0137039638
2017-09-20 20:25:48,776 - TensorMol - INFO - step:     250  duration: 50.64533  train loss: 0.0141434552  energy_loss: 0.0005010326  grad_loss: 0.0136424226
2017-09-20 20:25:58,634 - TensorMol - INFO - step:     250  duration: 9.85704  train loss: 0.0141137548  energy_loss: 0.0004732158  grad_loss: 0.0136405390
2017-09-20 20:26:48,968 - TensorMol - INFO - step:     251  duration: 50.01330  train loss: 0.0140295066  energy_loss: 0.0005018589  grad_loss: 0.0135276477
2017-09-20 20:27:38,648 - TensorMol - INFO - step:     252  duration: 49.67807  train loss: 0.0139022717  energy_loss: 0.0004950299  grad_loss: 0.0134072418
2017-09-20 20:28:28,587 - TensorMol - INFO - step:     253  duration: 49.93872  train loss: 0.0138238668  energy_loss: 0.0004873913  grad_loss: 0.0133364754
2017-09-20 20:29:19,291 - TensorMol - INFO - step:     254  duration: 50.70241  train loss: 0.0137059448  energy_loss: 0.0004792937  grad_loss: 0.0132266510
2017-09-20 20:30:09,073 - TensorMol - INFO - step:     255  duration: 49.78008  train loss: 0.0135597349  energy_loss: 0.0004742057  grad_loss: 0.0130855292
2017-09-20 20:30:58,375 - TensorMol - INFO - step:     256  duration: 49.30101  train loss: 0.0134560719  energy_loss: 0.0004784248  grad_loss: 0.0129776471
2017-09-20 20:31:47,976 - TensorMol - INFO - step:     257  duration: 49.59889  train loss: 0.0133802325  energy_loss: 0.0004635913  grad_loss: 0.0129166412
2017-09-20 20:32:37,692 - TensorMol - INFO - step:     258  duration: 49.71556  train loss: 0.0132203880  energy_loss: 0.0004647417  grad_loss: 0.0127556463
2017-09-20 20:33:27,523 - TensorMol - INFO - step:     259  duration: 49.82961  train loss: 0.0131460198  energy_loss: 0.0004488654  grad_loss: 0.0126971545
2017-09-20 20:34:17,002 - TensorMol - INFO - step:     260  duration: 49.47713  train loss: 0.0130449601  energy_loss: 0.0004501582  grad_loss: 0.0125948019
2017-09-20 20:34:27,231 - TensorMol - INFO - step:     260  duration: 10.22786  train loss: 0.0130489694  energy_loss: 0.0004164934  grad_loss: 0.0126324760
2017-09-20 20:35:16,860 - TensorMol - INFO - step:     261  duration: 49.30592  train loss: 0.0129310842  energy_loss: 0.0004432385  grad_loss: 0.0124878457
2017-09-20 20:36:06,456 - TensorMol - INFO - step:     262  duration: 49.59396  train loss: 0.0128731295  energy_loss: 0.0004371551  grad_loss: 0.0124359744
2017-09-20 20:36:56,488 - TensorMol - INFO - step:     263  duration: 50.03058  train loss: 0.0127842732  energy_loss: 0.0004290163  grad_loss: 0.0123552569
2017-09-20 20:37:46,117 - TensorMol - INFO - step:     264  duration: 49.62796  train loss: 0.0126611658  energy_loss: 0.0004248496  grad_loss: 0.0122363162
2017-09-20 20:38:35,693 - TensorMol - INFO - step:     265  duration: 49.57455  train loss: 0.0125600337  energy_loss: 0.0004296579  grad_loss: 0.0121303758
2017-09-20 20:39:26,404 - TensorMol - INFO - step:     266  duration: 50.70985  train loss: 0.0124856740  energy_loss: 0.0004162540  grad_loss: 0.0120694200
2017-09-20 20:40:15,967 - TensorMol - INFO - step:     267  duration: 49.56161  train loss: 0.0123564571  energy_loss: 0.0004182943  grad_loss: 0.0119381629
2017-09-20 20:41:06,490 - TensorMol - INFO - step:     268  duration: 50.52149  train loss: 0.0122879478  energy_loss: 0.0004043607  grad_loss: 0.0118835871
2017-09-20 20:41:56,478 - TensorMol - INFO - step:     269  duration: 49.98642  train loss: 0.0122005940  energy_loss: 0.0004067645  grad_loss: 0.0117938295
2017-09-20 20:42:46,004 - TensorMol - INFO - step:     270  duration: 49.52529  train loss: 0.0121240647  energy_loss: 0.0004007262  grad_loss: 0.0117233385
2017-09-20 20:42:55,963 - TensorMol - INFO - step:     270  duration: 9.95777  train loss: 0.0121947445  energy_loss: 0.0003720935  grad_loss: 0.0118226509
2017-09-20 20:43:46,092 - TensorMol - INFO - step:     271  duration: 49.66285  train loss: 0.0121133666  energy_loss: 0.0003958828  grad_loss: 0.0117174838
2017-09-20 20:44:35,603 - TensorMol - INFO - step:     272  duration: 49.51006  train loss: 0.0120426783  energy_loss: 0.0003880003  grad_loss: 0.0116546780
2017-09-20 20:45:25,456 - TensorMol - INFO - step:     273  duration: 49.85161  train loss: 0.0119574894  energy_loss: 0.0003848209  grad_loss: 0.0115726684
2017-09-20 20:46:15,947 - TensorMol - INFO - step:     274  duration: 50.49025  train loss: 0.0118806469  energy_loss: 0.0003899724  grad_loss: 0.0114906745
2017-09-20 20:47:05,943 - TensorMol - INFO - step:     275  duration: 49.99384  train loss: 0.0118286735  energy_loss: 0.0003780108  grad_loss: 0.0114506626
2017-09-20 20:47:55,588 - TensorMol - INFO - step:     276  duration: 49.64415  train loss: 0.0117439510  energy_loss: 0.0003810561  grad_loss: 0.0113628949
2017-09-20 20:48:46,281 - TensorMol - INFO - step:     277  duration: 50.69142  train loss: 0.0116760071  energy_loss: 0.0003691902  grad_loss: 0.0113068169
2017-09-20 20:49:36,081 - TensorMol - INFO - step:     278  duration: 49.79874  train loss: 0.0116013558  energy_loss: 0.0003723461  grad_loss: 0.0112290097
2017-09-20 20:50:25,325 - TensorMol - INFO - step:     279  duration: 49.24227  train loss: 0.0115271589  energy_loss: 0.0003665209  grad_loss: 0.0111606380
2017-09-20 20:51:15,671 - TensorMol - INFO - step:     280  duration: 50.34502  train loss: 0.0115000240  energy_loss: 0.0003624344  grad_loss: 0.0111375896
2017-09-20 20:51:25,599 - TensorMol - INFO - step:     280  duration: 9.92723  train loss: 0.0114992873  energy_loss: 0.0003362149  grad_loss: 0.0111630724
2017-09-20 20:52:15,455 - TensorMol - INFO - step:     281  duration: 49.62131  train loss: 0.0114366167  energy_loss: 0.0003550351  grad_loss: 0.0110815816
2017-09-20 20:53:05,350 - TensorMol - INFO - step:     282  duration: 49.89412  train loss: 0.0113434575  energy_loss: 0.0003520857  grad_loss: 0.0109913718
2017-09-20 20:53:55,291 - TensorMol - INFO - step:     283  duration: 49.93957  train loss: 0.0112649086  energy_loss: 0.0003564705  grad_loss: 0.0109084380
2017-09-20 20:54:45,974 - TensorMol - INFO - step:     284  duration: 50.68082  train loss: 0.0112206497  energy_loss: 0.0003446892  grad_loss: 0.0108759604
2017-09-20 20:55:36,275 - TensorMol - INFO - step:     285  duration: 50.29973  train loss: 0.0111149630  energy_loss: 0.0003480512  grad_loss: 0.0107669118
2017-09-20 20:56:26,270 - TensorMol - INFO - step:     286  duration: 49.99421  train loss: 0.0110328004  energy_loss: 0.0003371357  grad_loss: 0.0106956647
2017-09-20 20:57:15,416 - TensorMol - INFO - step:     287  duration: 49.14472  train loss: 0.0109562041  energy_loss: 0.0003402526  grad_loss: 0.0106159516
2017-09-20 20:58:04,943 - TensorMol - INFO - step:     288  duration: 49.52659  train loss: 0.0108759138  energy_loss: 0.0003360887  grad_loss: 0.0105398251
2017-09-20 20:58:54,135 - TensorMol - INFO - step:     289  duration: 49.19075  train loss: 0.0108240947  energy_loss: 0.0003339647  grad_loss: 0.0104901300
2017-09-20 20:59:43,526 - TensorMol - INFO - step:     290  duration: 49.38928  train loss: 0.0107560750  energy_loss: 0.0003276906  grad_loss: 0.0104283844
2017-09-20 20:59:53,490 - TensorMol - INFO - step:     290  duration: 9.96286  train loss: 0.0107566074  energy_loss: 0.0003084178  grad_loss: 0.0104481896
2017-09-20 21:00:43,514 - TensorMol - INFO - step:     291  duration: 49.64402  train loss: 0.0106697273  energy_loss: 0.0003258668  grad_loss: 0.0103438605
2017-09-20 21:01:33,158 - TensorMol - INFO - step:     292  duration: 49.64184  train loss: 0.0105887022  energy_loss: 0.0003310232  grad_loss: 0.0102576790
2017-09-20 21:02:22,683 - TensorMol - INFO - step:     293  duration: 49.52323  train loss: 0.0105460180  energy_loss: 0.0003209869  grad_loss: 0.0102250311
2017-09-20 21:03:12,801 - TensorMol - INFO - step:     294  duration: 50.11691  train loss: 0.0104510012  energy_loss: 0.0003248245  grad_loss: 0.0101261766
2017-09-20 21:04:02,486 - TensorMol - INFO - step:     295  duration: 49.68356  train loss: 0.0103898724  energy_loss: 0.0003148666  grad_loss: 0.0100750058
2017-09-20 21:04:52,035 - TensorMol - INFO - step:     296  duration: 49.54735  train loss: 0.0103376035  energy_loss: 0.0003181551  grad_loss: 0.0100194485
2017-09-20 21:05:41,972 - TensorMol - INFO - step:     297  duration: 49.93556  train loss: 0.0102689063  energy_loss: 0.0003147404  grad_loss: 0.0099541659
2017-09-20 21:06:31,654 - TensorMol - INFO - step:     298  duration: 49.68076  train loss: 0.0102319057  energy_loss: 0.0003132801  grad_loss: 0.0099186256
2017-09-20 21:07:20,957 - TensorMol - INFO - step:     299  duration: 49.30237  train loss: 0.0101910809  energy_loss: 0.0003076320  grad_loss: 0.0098834490
2017-09-20 21:08:10,291 - TensorMol - INFO - step:     300  duration: 49.33186  train loss: 0.0101248006  energy_loss: 0.0003059709  grad_loss: 0.0098188297
2017-09-20 21:08:20,128 - TensorMol - INFO - step:     300  duration: 9.83560  train loss: 0.0101211668  energy_loss: 0.0002897136  grad_loss: 0.0098314532
2017-09-20 21:09:09,979 - TensorMol - INFO - step:     301  duration: 49.54818  train loss: 0.0100403559  energy_loss: 0.0003111120  grad_loss: 0.0097292439
2017-09-20 21:10:00,166 - TensorMol - INFO - step:     302  duration: 50.18580  train loss: 0.0100113800  energy_loss: 0.0003019315  grad_loss: 0.0097094486
2017-09-20 21:10:50,013 - TensorMol - INFO - step:     303  duration: 49.84547  train loss: 0.0099255688  energy_loss: 0.0003061033  grad_loss: 0.0096194655
2017-09-20 21:11:39,658 - TensorMol - INFO - step:     304  duration: 49.64417  train loss: 0.0098643653  energy_loss: 0.0002972764  grad_loss: 0.0095670889
2017-09-20 21:12:29,339 - TensorMol - INFO - step:     305  duration: 49.67955  train loss: 0.0097960801  energy_loss: 0.0003003523  grad_loss: 0.0094957277
2017-09-20 21:13:19,538 - TensorMol - INFO - step:     306  duration: 50.19725  train loss: 0.0097316349  energy_loss: 0.0002972176  grad_loss: 0.0094344173
2017-09-20 21:14:09,051 - TensorMol - INFO - step:     307  duration: 49.51101  train loss: 0.0097435812  energy_loss: 0.0002962790  grad_loss: 0.0094473022
2017-09-20 21:14:58,926 - TensorMol - INFO - step:     308  duration: 49.87371  train loss: 0.0097282148  energy_loss: 0.0002916705  grad_loss: 0.0094365443
2017-09-20 21:15:48,267 - TensorMol - INFO - step:     309  duration: 49.33974  train loss: 0.0096875814  energy_loss: 0.0002902588  grad_loss: 0.0093973226
2017-09-20 21:16:38,077 - TensorMol - INFO - step:     310  duration: 49.80905  train loss: 0.0096269103  energy_loss: 0.0002956982  grad_loss: 0.0093312121
2017-09-20 21:16:47,955 - TensorMol - INFO - step:     310  duration: 9.87651  train loss: 0.0096371551  energy_loss: 0.0002767343  grad_loss: 0.0093604208
2017-09-20 21:17:37,454 - TensorMol - INFO - step:     311  duration: 49.18553  train loss: 0.0095992424  energy_loss: 0.0002873884  grad_loss: 0.0093118541
2017-09-20 21:18:27,038 - TensorMol - INFO - step:     312  duration: 49.58174  train loss: 0.0095307758  energy_loss: 0.0002915795  grad_loss: 0.0092391963
2017-09-20 21:19:17,248 - TensorMol - INFO - step:     313  duration: 50.20963  train loss: 0.0094751672  energy_loss: 0.0002830609  grad_loss: 0.0091921064
2017-09-20 21:20:06,807 - TensorMol - INFO - step:     314  duration: 49.55783  train loss: 0.0094453444  energy_loss: 0.0002861540  grad_loss: 0.0091591904
2017-09-20 21:20:56,551 - TensorMol - INFO - step:     315  duration: 49.74323  train loss: 0.0093874758  energy_loss: 0.0002832541  grad_loss: 0.0091042217
2017-09-20 21:21:47,212 - TensorMol - INFO - step:     316  duration: 50.65950  train loss: 0.0093602193  energy_loss: 0.0002826739  grad_loss: 0.0090775454
2017-09-20 21:22:37,098 - TensorMol - INFO - step:     317  duration: 49.88423  train loss: 0.0093251568  energy_loss: 0.0002778698  grad_loss: 0.0090472870
2017-09-20 21:23:26,891 - TensorMol - INFO - step:     318  duration: 49.79207  train loss: 0.0092769471  energy_loss: 0.0002760171  grad_loss: 0.0090009300
2017-09-20 21:24:16,186 - TensorMol - INFO - step:     319  duration: 49.29374  train loss: 0.0092304513  energy_loss: 0.0002817252  grad_loss: 0.0089487261
2017-09-20 21:25:05,689 - TensorMol - INFO - step:     320  duration: 49.50165  train loss: 0.0092228081  energy_loss: 0.0002739155  grad_loss: 0.0089488926
2017-09-20 21:25:15,278 - TensorMol - INFO - step:     320  duration: 9.58836  train loss: 0.0091817972  energy_loss: 0.0002637266  grad_loss: 0.0089180706
2017-09-20 21:26:05,484 - TensorMol - INFO - step:     321  duration: 49.78982  train loss: 0.0091771313  energy_loss: 0.0002785627  grad_loss: 0.0088985686
2017-09-20 21:26:56,358 - TensorMol - INFO - step:     322  duration: 50.87197  train loss: 0.0091260259  energy_loss: 0.0002709093  grad_loss: 0.0088551166
2017-09-20 21:27:45,905 - TensorMol - INFO - step:     323  duration: 49.54572  train loss: 0.0090969729  energy_loss: 0.0002747131  grad_loss: 0.0088222597
2017-09-20 21:28:35,226 - TensorMol - INFO - step:     324  duration: 49.31923  train loss: 0.0090537057  energy_loss: 0.0002715589  grad_loss: 0.0087821468
2017-09-20 21:29:24,447 - TensorMol - INFO - step:     325  duration: 49.21988  train loss: 0.0090378122  energy_loss: 0.0002707842  grad_loss: 0.0087670280
2017-09-20 21:30:14,206 - TensorMol - INFO - step:     326  duration: 49.75821  train loss: 0.0090151218  energy_loss: 0.0002663224  grad_loss: 0.0087487994
2017-09-20 21:31:03,752 - TensorMol - INFO - step:     327  duration: 49.54431  train loss: 0.0089588395  energy_loss: 0.0002641355  grad_loss: 0.0086947039
2017-09-20 21:31:53,082 - TensorMol - INFO - step:     328  duration: 49.32946  train loss: 0.0089120508  energy_loss: 0.0002697329  grad_loss: 0.0086423178
2017-09-20 21:32:43,370 - TensorMol - INFO - step:     329  duration: 50.28648  train loss: 0.0088953451  energy_loss: 0.0002624598  grad_loss: 0.0086328853
2017-09-20 21:33:32,612 - TensorMol - INFO - step:     330  duration: 49.24156  train loss: 0.0088611877  energy_loss: 0.0002670566  grad_loss: 0.0085941311
2017-09-20 21:33:42,711 - TensorMol - INFO - step:     330  duration: 10.09676  train loss: 0.0087962170  energy_loss: 0.0002523871  grad_loss: 0.0085438299
2017-09-20 21:34:32,618 - TensorMol - INFO - step:     331  duration: 49.52228  train loss: 0.0088133104  energy_loss: 0.0002597785  grad_loss: 0.0085535319
2017-09-20 21:35:22,582 - TensorMol - INFO - step:     332  duration: 49.96203  train loss: 0.0087900374  energy_loss: 0.0002636510  grad_loss: 0.0085263864
2017-09-20 21:36:12,304 - TensorMol - INFO - step:     333  duration: 49.72117  train loss: 0.0087559974  energy_loss: 0.0002607793  grad_loss: 0.0084952181
2017-09-20 21:37:02,236 - TensorMol - INFO - step:     334  duration: 49.93108  train loss: 0.0087292513  energy_loss: 0.0002608352  grad_loss: 0.0084684162
2017-09-20 21:37:52,090 - TensorMol - INFO - step:     335  duration: 49.85183  train loss: 0.0087076603  energy_loss: 0.0002569856  grad_loss: 0.0084506747
2017-09-20 21:38:42,277 - TensorMol - INFO - step:     336  duration: 50.18631  train loss: 0.0086704444  energy_loss: 0.0002550095  grad_loss: 0.0084154349
2017-09-20 21:39:32,375 - TensorMol - INFO - step:     337  duration: 50.09699  train loss: 0.0086177369  energy_loss: 0.0002607404  grad_loss: 0.0083569965
2017-09-20 21:40:22,357 - TensorMol - INFO - step:     338  duration: 49.98043  train loss: 0.0086089048  energy_loss: 0.0002534700  grad_loss: 0.0083554348
2017-09-20 21:41:12,137 - TensorMol - INFO - step:     339  duration: 49.77929  train loss: 0.0085641259  energy_loss: 0.0002579797  grad_loss: 0.0083061463
2017-09-20 21:42:02,054 - TensorMol - INFO - step:     340  duration: 49.91476  train loss: 0.0085288927  energy_loss: 0.0002511202  grad_loss: 0.0082777726
2017-09-20 21:42:12,076 - TensorMol - INFO - step:     340  duration: 10.02015  train loss: 0.0085042663  energy_loss: 0.0002406331  grad_loss: 0.0082636331
2017-09-20 21:43:01,676 - TensorMol - INFO - step:     341  duration: 49.22947  train loss: 0.0085104467  energy_loss: 0.0002547933  grad_loss: 0.0082556534
2017-09-20 21:43:51,133 - TensorMol - INFO - step:     342  duration: 49.45555  train loss: 0.0084863426  energy_loss: 0.0002519033  grad_loss: 0.0082344393
2017-09-20 21:44:41,105 - TensorMol - INFO - step:     343  duration: 49.97116  train loss: 0.0084721266  energy_loss: 0.0002521522  grad_loss: 0.0082199744
2017-09-20 21:45:31,598 - TensorMol - INFO - step:     344  duration: 50.49111  train loss: 0.0084430811  energy_loss: 0.0002486131  grad_loss: 0.0081944680
2017-09-20 21:46:21,960 - TensorMol - INFO - step:     345  duration: 50.36101  train loss: 0.0084006999  energy_loss: 0.0002467930  grad_loss: 0.0081539069
2017-09-20 21:47:11,351 - TensorMol - INFO - step:     346  duration: 49.38944  train loss: 0.0083531758  energy_loss: 0.0002526211  grad_loss: 0.0081005547
2017-09-20 21:48:01,351 - TensorMol - INFO - step:     347  duration: 49.99855  train loss: 0.0083388663  energy_loss: 0.0002453213  grad_loss: 0.0080935450
2017-09-20 21:48:50,418 - TensorMol - INFO - step:     348  duration: 49.06555  train loss: 0.0083032531  energy_loss: 0.0002496181  grad_loss: 0.0080536350
2017-09-20 21:49:40,232 - TensorMol - INFO - step:     349  duration: 49.81308  train loss: 0.0082548215  energy_loss: 0.0002428535  grad_loss: 0.0080119680
2017-09-20 21:50:29,692 - TensorMol - INFO - step:     350  duration: 49.45816  train loss: 0.0082384473  energy_loss: 0.0002463181  grad_loss: 0.0079921292
2017-09-20 21:50:39,577 - TensorMol - INFO - step:     350  duration: 9.88323  train loss: 0.0082186100  energy_loss: 0.0002306616  grad_loss: 0.0079879484
2017-09-20 21:51:29,105 - TensorMol - INFO - step:     351  duration: 49.20923  train loss: 0.0081935389  energy_loss: 0.0002435271  grad_loss: 0.0079500118
2017-09-20 21:52:18,383 - TensorMol - INFO - step:     352  duration: 49.27657  train loss: 0.0081649007  energy_loss: 0.0002437748  grad_loss: 0.0079211259
2017-09-20 21:53:07,415 - TensorMol - INFO - step:     353  duration: 49.03023  train loss: 0.0081408092  energy_loss: 0.0002404175  grad_loss: 0.0079003917
2017-09-20 21:53:57,215 - TensorMol - INFO - step:     354  duration: 49.79966  train loss: 0.0080957719  energy_loss: 0.0002385583  grad_loss: 0.0078572135
2017-09-20 21:54:47,401 - TensorMol - INFO - step:     355  duration: 50.18550  train loss: 0.0080627250  energy_loss: 0.0002443984  grad_loss: 0.0078183266
2017-09-20 21:55:36,844 - TensorMol - INFO - step:     356  duration: 49.44071  train loss: 0.0080428139  energy_loss: 0.0002373788  grad_loss: 0.0078054351
2017-09-20 21:56:27,096 - TensorMol - INFO - step:     357  duration: 50.25084  train loss: 0.0079907702  energy_loss: 0.0002417815  grad_loss: 0.0077489886
2017-09-20 21:57:16,820 - TensorMol - INFO - step:     358  duration: 49.72266  train loss: 0.0079495973  energy_loss: 0.0002353309  grad_loss: 0.0077142664
2017-09-20 21:58:06,364 - TensorMol - INFO - step:     359  duration: 49.54277  train loss: 0.0079232908  energy_loss: 0.0002387041  grad_loss: 0.0076845867
2017-09-20 21:58:55,993 - TensorMol - INFO - step:     360  duration: 49.62680  train loss: 0.0078863538  energy_loss: 0.0002359637  grad_loss: 0.0076503902
2017-09-20 21:59:06,033 - TensorMol - INFO - step:     360  duration: 10.03893  train loss: 0.0078682534  energy_loss: 0.0002238444  grad_loss: 0.0076444090
2017-09-20 21:59:55,609 - TensorMol - INFO - step:     361  duration: 49.23035  train loss: 0.0078594385  energy_loss: 0.0002362213  grad_loss: 0.0076232173
2017-09-20 22:00:45,408 - TensorMol - INFO - step:     362  duration: 49.79804  train loss: 0.0078372477  energy_loss: 0.0002332509  grad_loss: 0.0076039968
2017-09-20 22:01:34,800 - TensorMol - INFO - step:     363  duration: 49.39151  train loss: 0.0077938906  energy_loss: 0.0002313978  grad_loss: 0.0075624928
2017-09-20 22:02:24,353 - TensorMol - INFO - step:     364  duration: 49.55138  train loss: 0.0077681213  energy_loss: 0.0002371167  grad_loss: 0.0075310046
2017-09-20 22:03:14,207 - TensorMol - INFO - step:     365  duration: 49.85297  train loss: 0.0077529130  energy_loss: 0.0002303617  grad_loss: 0.0075225513
2017-09-20 22:04:03,784 - TensorMol - INFO - step:     366  duration: 49.57599  train loss: 0.0077070582  energy_loss: 0.0002346604  grad_loss: 0.0074723978
2017-09-20 22:04:53,315 - TensorMol - INFO - step:     367  duration: 49.52917  train loss: 0.0076799360  energy_loss: 0.0002282200  grad_loss: 0.0074517159
2017-09-20 22:05:43,078 - TensorMol - INFO - step:     368  duration: 49.76220  train loss: 0.0076610043  energy_loss: 0.0002314392  grad_loss: 0.0074295652
2017-09-20 22:06:32,915 - TensorMol - INFO - step:     369  duration: 49.83541  train loss: 0.0076252847  energy_loss: 0.0002288232  grad_loss: 0.0073964615
2017-09-20 22:07:22,364 - TensorMol - INFO - step:     370  duration: 49.44797  train loss: 0.0076095244  energy_loss: 0.0002290643  grad_loss: 0.0073804601
2017-09-20 22:07:32,168 - TensorMol - INFO - step:     370  duration: 9.80230  train loss: 0.0076065217  energy_loss: 0.0002178235  grad_loss: 0.0073886982
2017-09-20 22:08:21,972 - TensorMol - INFO - step:     371  duration: 49.54627  train loss: 0.0075876946  energy_loss: 0.0002263224  grad_loss: 0.0073613721
2017-09-20 22:09:11,772 - TensorMol - INFO - step:     372  duration: 49.79841  train loss: 0.0075542509  energy_loss: 0.0002244879  grad_loss: 0.0073297630
2017-09-20 22:10:01,495 - TensorMol - INFO - step:     373  duration: 49.72093  train loss: 0.0075235247  energy_loss: 0.0002299257  grad_loss: 0.0072935991
2017-09-20 22:10:51,241 - TensorMol - INFO - step:     374  duration: 49.74483  train loss: 0.0075184141  energy_loss: 0.0002234287  grad_loss: 0.0072949854
2017-09-20 22:11:40,407 - TensorMol - INFO - step:     375  duration: 49.16457  train loss: 0.0074763341  energy_loss: 0.0002276368  grad_loss: 0.0072486973
2017-09-20 22:12:29,625 - TensorMol - INFO - step:     376  duration: 49.21690  train loss: 0.0074411261  energy_loss: 0.0002213201  grad_loss: 0.0072198060
2017-09-20 22:13:19,145 - TensorMol - INFO - step:     377  duration: 49.51867  train loss: 0.0074227506  energy_loss: 0.0002246518  grad_loss: 0.0071980988
2017-09-20 22:14:09,802 - TensorMol - INFO - step:     378  duration: 50.65512  train loss: 0.0073928374  energy_loss: 0.0002221018  grad_loss: 0.0071707357
2017-09-20 22:14:58,916 - TensorMol - INFO - step:     379  duration: 49.11302  train loss: 0.0073820995  energy_loss: 0.0002222261  grad_loss: 0.0071598734
2017-09-20 22:15:48,451 - TensorMol - INFO - step:     380  duration: 49.53356  train loss: 0.0073599759  energy_loss: 0.0002195741  grad_loss: 0.0071404018
2017-09-20 22:15:58,377 - TensorMol - INFO - step:     380  duration: 9.92450  train loss: 0.0073438376  energy_loss: 0.0002081599  grad_loss: 0.0071356777
2017-09-20 22:16:48,594 - TensorMol - INFO - step:     381  duration: 49.89827  train loss: 0.0073280215  energy_loss: 0.0002176652  grad_loss: 0.0071103563
2017-09-20 22:17:38,653 - TensorMol - INFO - step:     382  duration: 50.05736  train loss: 0.0072982285  energy_loss: 0.0002228945  grad_loss: 0.0070753340
2017-09-20 22:18:28,629 - TensorMol - INFO - step:     383  duration: 49.97488  train loss: 0.0072887879  energy_loss: 0.0002166995  grad_loss: 0.0070720885
2017-09-20 22:19:18,754 - TensorMol - INFO - step:     384  duration: 50.12365  train loss: 0.0072582094  energy_loss: 0.0002206331  grad_loss: 0.0070375763
2017-09-20 22:20:08,949 - TensorMol - INFO - step:     385  duration: 50.19380  train loss: 0.0072322093  energy_loss: 0.0002142311  grad_loss: 0.0070179781
2017-09-20 22:20:58,563 - TensorMol - INFO - step:     386  duration: 49.61209  train loss: 0.0072197182  energy_loss: 0.0002173781  grad_loss: 0.0070023401
2017-09-20 22:21:47,909 - TensorMol - INFO - step:     387  duration: 49.34563  train loss: 0.0071964546  energy_loss: 0.0002147916  grad_loss: 0.0069816630
2017-09-20 22:22:37,580 - TensorMol - INFO - step:     388  duration: 49.66900  train loss: 0.0071805707  energy_loss: 0.0002147269  grad_loss: 0.0069658437
2017-09-20 22:23:27,691 - TensorMol - INFO - step:     389  duration: 50.11030  train loss: 0.0071611477  energy_loss: 0.0002119109  grad_loss: 0.0069492368
2017-09-20 22:24:17,093 - TensorMol - INFO - step:     390  duration: 49.40053  train loss: 0.0071359018  energy_loss: 0.0002098163  grad_loss: 0.0069260855
2017-09-20 22:24:27,051 - TensorMol - INFO - step:     390  duration: 9.95654  train loss: 0.0071313258  energy_loss: 0.0002021108  grad_loss: 0.0069292151
2017-09-20 22:25:17,423 - TensorMol - INFO - step:     391  duration: 50.01934  train loss: 0.0071123574  energy_loss: 0.0002148852  grad_loss: 0.0068974722
2017-09-20 22:26:07,306 - TensorMol - INFO - step:     392  duration: 49.88068  train loss: 0.0071055808  energy_loss: 0.0002090458  grad_loss: 0.0068965350
2017-09-20 22:26:56,672 - TensorMol - INFO - step:     393  duration: 49.36486  train loss: 0.0070703025  energy_loss: 0.0002125409  grad_loss: 0.0068577616
2017-09-20 22:27:46,129 - TensorMol - INFO - step:     394  duration: 49.45609  train loss: 0.0070296096  energy_loss: 0.0002061399  grad_loss: 0.0068234697
2017-09-20 22:28:35,462 - TensorMol - INFO - step:     395  duration: 49.33101  train loss: 0.0070123692  energy_loss: 0.0002091225  grad_loss: 0.0068032466
2017-09-20 22:29:24,917 - TensorMol - INFO - step:     396  duration: 49.45471  train loss: 0.0069876327  energy_loss: 0.0002067658  grad_loss: 0.0067808668
2017-09-20 22:30:14,048 - TensorMol - INFO - step:     397  duration: 49.12951  train loss: 0.0069691475  energy_loss: 0.0002068526  grad_loss: 0.0067622949
2017-09-20 22:31:03,547 - TensorMol - INFO - step:     398  duration: 49.49801  train loss: 0.0069458879  energy_loss: 0.0002041214  grad_loss: 0.0067417665
2017-09-20 22:31:53,657 - TensorMol - INFO - step:     399  duration: 50.10887  train loss: 0.0069129604  energy_loss: 0.0002019969  grad_loss: 0.0067109636
2017-09-20 22:32:43,295 - TensorMol - INFO - step:     400  duration: 49.63690  train loss: 0.0068874369  energy_loss: 0.0002066907  grad_loss: 0.0066807462
2017-09-20 22:32:53,100 - TensorMol - INFO - step:     400  duration: 9.80333  train loss: 0.0069169170  energy_loss: 0.0002005262  grad_loss: 0.0067163908
2017-09-20 22:32:53,471 - TensorMol - INFO - ~ Adios Homeshake ~
